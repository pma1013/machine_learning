# ポイント

* 各変数の正規化が重要(標準偏回帰係数)
  * 各変数がどの程度目的変数に影響しているかを確認するには、各変数を正規化 (標準化) し、
平均 = 0, 標準偏差 = 1 になるように変換した上で、
重回帰分析を行うと偏回帰係数の大小で比較することができるようになります。
  * 標準偏回帰係数は重回帰式における各変数の重要性を表す指標

* 不均衡なデータ(特徴量の尺度違い)の分類問題をどうとくか？
http://kamonohashiperry.com/archives/469
  * アプローチ方法は2つある
https://www.slideshare.net/sfchaos/ss-11307051
    * 誤答した時のペナルティを重くする(cost-sensitive learning)
    * それぞれのサンプル数を調整する
  * ランダムフォレストでのアプローチ方法
    * Weighted Random Forest
      正例、負例をそれぞれ誤って分類する際のペナルティを以下の2箇所で考慮する
      * Gini係数を評価基準として決定木の枝を作成するとき
      * 予測ラベルを決定する際に重み付き多数決を取るとき
    * Balanced Random Forest
      各ツリーを構築する際、正例のデータ数と同じだけ負例のデータをサンプリングして学習する
* 不均衡データの整形
http://drilldripper.hatenablog.com/entry/2016/09/01/181259

## 過学習

* 過学習の原因は与えられたトレーニングデータセットに対してモデルが複雑すぎることにある
* 対応策としては以下の通り
  * さらに多くのトレーニングデータを集める
  * 正規化を通じて複雑さにペナルティを科す
  * パラメータの数が少ない、より単純なモデルを選択する
  * データの次元の数を減らす

## 特徴量の重要度

* ランダムフォレストで重要な特徴量が選択可能
* ランダムフォレストでは特徴量を標準化または正規化する必要はない

## データの特徴を知る
http://qiita.com/shu_marubo/items/a6fa8cd95c2f8a82b68c  
実際にモデルを作成、採択をする際には、データの前処理、モデル作成、モデル評価を行う必要があり、データの様子や、モデルの使用環境に応じた評価方法を取る必要がある。

* 概要
wine_df.describe()
* 欠損値の確認
* カテゴリカル変数の確認
wine_df.head()
目視で確認
** カテゴリカル変数とは
「性別」とか「社会人か学生か」のように離散値であり大小関係がない値からなる変数

# 可視化

* データがどういった特性(偏り)があるかを判別するにはSeabornを使った可視化が良さそう
  * その①
http://qiita.com/hik0107/items/3dc541158fceb3156ee0
  * その②
http://qiita.com/hik0107/items/7233ca334b2a5e1ca924

# 用語

* 説明変数と目的変数
  * 説明変数
  目的変数を説明する変数のこと。要は導きたいラベルの要素となるもの
  * 目的変数
  予測したい変数のこと。導きたいラベルそのもの
  * 例)
    * ①「身長・体重・年齢・摂取カロリー」などを説明変数とし 「血清コレステロール値」を目的変数とする
    * ②「筋肉量・体脂肪率・身長」などを説明変数とし 「男であるか女であるか」を目的変数とする

* 単回帰分析
1つの説明変数から目的変数を導き出す
* 重回帰分析
複数の説明変数から目的変数を導き出す
* アンダーサンプリング
不均衡データを正規化するための手法の1つ
データが多い方のクラスを少なくする手法
* オーバーサンプリング
不均衡データを正規化するための手法の1つ
データが小さい方のクラスを多くする手法

# 参考

* リクルートコミュニケーションの例
https://www.slideshare.net/takashijozaki1/taste-of-wine-vs-data-science
